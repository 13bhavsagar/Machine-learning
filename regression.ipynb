{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Simple Linear Regression?\n",
        "- Simple Linear Regression is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y).\n",
        "It fits a straight line, defined by the equation Y = a + bX, where a is the intercept and b is the slope.\n",
        "The goal is to predict or understand how changes in X affect Y by minimizing the difference between predicted and actual values.\n",
        "\n",
        "2.What are the key assumptions of Simple Linear Regression?\n",
        "- Linearity: The relationship between the independent variable (X) and dependent variable (Y) is linear.\n",
        "\n",
        "- Independence: The residuals (errors) are independent of each other.\n",
        "\n",
        "- Homoscedasticity: The residuals have constant variance at every level of X.\n",
        "\n",
        "- Normality: The residuals are normally distributed (especially important for inference).\n",
        "\n",
        "- No multicollinearity: (Applies to multiple regression) — in simple linear regression, this is not relevant as there's only one predictor.\n",
        "\n",
        "3.What does the coefficient m represent in the equation Y=mX+c?\n",
        "- In the equation Y = mX + c, the coefficient m represents the slope, showing the change in Y for a one-unit change in\n",
        "\n",
        "4.What does the intercept c represent in the equation Y=mX+c?\n",
        "- In the equation Y = mX + c, the intercept c represents the value of Y when X is zero.\n",
        "\n",
        "5.How do we calculate the slope m in Simple Linear Regression?\n",
        "- $$\n",
        "m = \\frac{n\\sum XY - \\sum X \\sum Y}{n\\sum X^2 - (\\sum X)^2}\n",
        "$$\n",
        "- $$\n",
        "\\begin{aligned}\n",
        "&n \\text{ is the number of data points} \\\\\n",
        "&\\sum XY \\text{ is the sum of the product of corresponding } X \\text{ and } Y \\text{ values} \\\\\n",
        "&\\sum X \\text{ and } \\sum Y \\text{ are the sums of the } X \\text{ and } Y \\text{ values respectively} \\\\\n",
        "&\\sum X^2 \\text{ is the sum of squares of } X \\text{ values}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "- The purpose of the least squares method in Simple Linear Regression is to find the best-fitting straight line through the data by minimizing the sum of the squared differences (errors) between the actual values (Y) and the predicted value.\n",
        "\n",
        "7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "- The coefficient of determination (R²) in Simple Linear Regression indicates the proportion of the variance in the dependent variable (Y) that is predictable from the independent variable (X).\n",
        "An R² value of 1 means the model perfectly fits the data, while an R² of 0 means it explains none of the variability.\n",
        "\n",
        "8.What is Multiple Linear Regression?\n",
        "- Multiple Linear Regression is a statistical method used to model the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, ..., Xₙ).\n",
        "\n",
        "9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "- The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable. In Simple Linear Regression, there is only one independent variable and one dependent variable, and the goal is to model their linear relationship. In contrast, Multiple Linear Regression involves two or more independent variables used to predict a single dependent variable. This allows the model to capture more complex relationships by considering the combined effect of multiple predictors on the outcome.\n",
        "\n",
        "10.What are the key assumptions of Multiple Linear Regression?\n",
        "- Linearity: There is a linear relationship between the dependent variable and each independent variable.\n",
        "\n",
        "- Independence: The residuals (errors) are independent across observations.\n",
        "\n",
        "- Homoscedasticity: The residuals have constant variance across all levels of the independent variables.\n",
        "\n",
        "- Normality: The residuals are normally distributed, especially for reliable hypothesis testing.\n",
        "\n",
        "- No Multicollinearity: The independent variables are not highly correlated with each other.\n",
        "\n",
        "- No Autocorrelation: Especially in time-series data, the residuals should not be correlated with each other.\n",
        "\n",
        "\n",
        "11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "- Heteroscedasticity refers to a condition in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables. In other words, as the value of the predictors changes, the spread or \"scatter\" of the errors changes too.\n",
        "\n",
        "12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "- To improve a Multiple Linear Regression model with high multicollinearity, you can start by removing or combining highly correlated predictors to reduce redundancy. Applying techniques like Ridge or Lasso Regression can also help, as they penalize large coefficients and stabilize the model. Checking the Variance Inflation Factor (VIF) helps identify problematic variables. Additionally, collecting more data or centering the variables can further minimize the impact of multicollinearity.\n",
        "\n",
        "13.What are some common techniques for transforming categorical variables for use in regression models?\n",
        "- One-Hot Encoding – Converts each category into separate binary columns (0 or 1); suitable for nominal (unordered) categories.\n",
        "\n",
        "- Label Encoding – Assigns a unique integer to each category; best for ordinal (ordered) categories but can mislead if used on nominal data.\n",
        "\n",
        "- Ordinal Encoding – Preserves the order by mapping categories to ranked integers; ideal for features with natural order.\n",
        "\n",
        "- Binary Encoding – Encodes categories into binary format and splits them into multiple columns; efficient for high-cardinality variables.\n",
        "\n",
        "14.What is the role of interaction terms in Multiple Linear Regression?\n",
        "- In Multiple Linear Regression, interaction terms capture the combined effect of two or more independent variables on the dependent variable that is not explained by their individual effects alone.\n",
        "\n",
        "- They help model situations where the impact of one variable on the outcome depends on the value of another variable. For example, the effect of education on income might depend on years of experience — this relationship is captured through an interaction term (e.g., Education × Experience). Including interaction terms can improve model accuracy when such combined effects are present in the data.\n",
        "\n",
        "15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "- In Simple Linear Regression, the intercept represents the expected value of the dependent variable Y when the single independent variable X is zero. It often has a clear, straightforward interpretation.\n",
        "\n",
        "- In Multiple Linear Regression, the intercept is the expected value of Y when all independent variables are zero simultaneously. This can be less meaningful or even unrealistic if a combination of all predictors being zero doesn't make practical sense. Therefore, interpretation depends heavily on the context and the scale of the predictors.\n",
        "\n",
        "16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "- In regression analysis, the slope represents the change in the dependent variable (Y) for a one-unit increase in an independent variable (X), assuming all other variables remain constant (in multiple regression).\n",
        "\n",
        "- It indicates the strength and direction of the relationship: a positive slope suggests that as X increases, Y increases, while a negative slope means Y decreases. The slope directly affects predictions because it determines how changes in input values influence the predicted output. Accurate slope estimates are essential for reliable forecasting and understanding variable impact.\n",
        "\n",
        "17.How does the intercept in a regression model provide context for the relationship between variables?\n",
        "- The intercept in a regression model provides the baseline value of the dependent variable (Y) when all independent variables (X) are zero. It helps anchor the regression line or plane and gives context for how other variables contribute to changes in Y.\n",
        "\n",
        "18.What are the limitations of using R² as a sole measure of model performance?\n",
        "- Doesn’t Indicate Causation: A high R² doesn't mean the model explains a causal relationship—just correlation.\n",
        "\n",
        "- Insensitive to Overfitting: R² always increases (or stays the same) when more predictors are added, even if they don't improve the model meaningfully.\n",
        "\n",
        "- No Information About Bias or Error: It doesn’t reflect whether predictions are systematically too high or low.\n",
        "\n",
        "- Not Useful for Nonlinear Models: R² can be misleading when applied to models that don’t assume linearity.\n",
        "\n",
        "19.How would you interpret a large standard error for a regression coefficient?\n",
        "- A large standard error for a regression coefficient suggests that the estimate of that coefficient is highly uncertain or unstable.This means the model may not reliably determine the true relationship between the independent and dependent variable. It often indicates that the predictor variable may not be significantly contributing to the model, or that issues like multicollinearity, small sample size, or high variance in the data are present. As a result, the corresponding t-statistic will be smaller, potentially leading to non-significant p-values, making it harder to confidently conclude that the variable has a real effect.\n",
        "\n",
        "20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "- Heteroscedasticity can be identified in residual plots when the residuals (errors) display a pattern or spread that changes with the fitted values. A common sign is a funnel shape—where residuals fan out or contract as the predicted values increase—indicating that the variance is not constant.\n",
        "\n",
        "- It is important to address heteroscedasticity because it violates a key regression assumption (constant variance of errors), which can lead to biased standard errors, unreliable confidence intervals, and invalid hypothesis tests. Addressing it ensures more accurate and trustworthy model interpretations.\n",
        "\n",
        "21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "- If a Multiple Linear Regression model has a high R² but a low adjusted R², it means that adding more independent variables increased R² (which always happens or stays the same), but those additional variables did not actually improve the model in a meaningful way.\n",
        "\n",
        "22.Why is it important to scale variables in Multiple Linear Regression?\n",
        "- Improves Interpretability in Regularization: In models like Ridge or Lasso regression, unscaled features can lead to biased coefficient shrinkage since regularization penalizes large coefficients, and larger-scale features dominate the penalty.\n",
        "\n",
        "- Prevents Numerical Instability: Very large or very small feature values can cause computational issues, leading to inaccurate results or failure to converge.\n",
        "\n",
        "- Balances Feature Influence: Without scaling, features with larger ranges can disproportionately influence the model, even if they aren’t more important.\n",
        "\n",
        "- Enables Fair Comparison of Coefficients: When variables are on the same scale, you can better compare their relative effects on the dependent variable.\n",
        "\n",
        "23.What is polynomial regression?\n",
        "- Polynomial regression is a type of regression analysis where the relationship between the independent variable X and the dependent variable\n",
        "Y is modeled as an nth-degree polynomial. It extends linear regression by allowing the model to fit nonlinear data patterns while still being linear in terms of the coefficients.\n",
        "\n",
        "24.How does polynomial regression differ from linear regression?\n",
        "- Polynomial regression differs from linear regression in that it can model nonlinear relationships between the independent and dependent variables by including higher-degree terms of the predictor variable. While linear regression assumes a straight-line relationship, polynomial regression fits a curved line to the data, making it more suitable for datasets where trends change direction. Despite this flexibility, polynomial regression remains linear in terms of the model coefficients, which allows it to be solved using linear regression techniques.\n",
        "\n",
        "25.When is polynomial regression used?\n",
        "- Polynomial regression is used when the relationship between the independent variable and the dependent variable is nonlinear and cannot be accurately captured by a straight line. It is especially useful when data shows a curved trend, such as U-shapes, inverted U-shapes, or more complex curves.\n",
        "\n",
        "26.What is the general equation for polynomial regression?\n",
        "- $$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n + \\varepsilon\n",
        "$$\n",
        "\n",
        "27.Can polynomial regression be applied to multiple variables?\n",
        "- Yes, polynomial regression can be applied to multiple variables, and this is known as multivariate polynomial regression or polynomial regression with multiple predictors.\n",
        "\n",
        "28.What are the limitations of polynomial regression?\n",
        "- Overfitting: Higher-degree polynomials can fit the training data very closely but generalize poorly to new data.\n",
        "\n",
        "- Extrapolation Issues: Predictions outside the range of the training data can be highly unreliable and unstable.\n",
        "\n",
        "- Interpretability: As the degree increases, the model becomes complex and the coefficients lose intuitive meaning.\n",
        "\n",
        "- Computational Cost: With multiple variables and high degrees, the number of terms grows rapidly, increasing computation and risk of multicollinearity.\n",
        "\n",
        "- Sensitive to Outliers: Polynomial regression can be highly influenced by outliers, which may distort the curve significantly.\n",
        "\n",
        "29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- Cross-Validation (e.g., k-Fold CV) – Evaluates model performance on unseen data to find the degree that generalizes best.\n",
        "\n",
        "- Adjusted R² – Unlike R², it penalizes the addition of unnecessary polynomial terms, helping identify the optimal complexity.\n",
        "\n",
        "- Mean Squared Error (MSE) / Root Mean Squared Error (RMSE) – Lower values on validation/test sets indicate better fit.\n",
        "\n",
        "30.Why is visualization important in polynomial regression?\n",
        "- Visualization is important in polynomial regression as it helps assess how well the model fits the data, especially when capturing nonlinear trends. It allows us to detect overfitting or underfitting by examining the shape of the fitted curve. Residual plots from visualization can reveal patterns that indicate poor model assumptions. Additionally, it aids in clearly communicating model behavior and performance to both technical and non-technical audiences.\n",
        "\n",
        "31.How is polynomial regression implemented in Python?\n",
        "- import numpy as np\n",
        "- import matplotlib.pyplot as plt\n",
        "- from sklearn.linear_model importLinearRegression\n",
        "- from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "- X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "- y = np.array([2, 5, 10, 17, 26])\n",
        "\n",
        "- poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "- X_poly = poly.fit_transform(X)\n",
        "\n",
        "- model = LinearRegression()\n",
        "- model.fit(X_poly, y)\n",
        "\n",
        "- y_pred = model.predict(X_poly)\n",
        "\n",
        "- plt.scatter(X, y, color='blue',   label='Original Data')\n",
        "-  plt.plot(X, y_pred, color='red', label='Polynomial Fit')\n",
        "- plt.legend()\n",
        "- plt.xlabel('X')\n",
        "-  plt.ylabel('y')\n",
        "-  plt.title('Polynomial Regression')\n",
        "-  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NLAv8O9FHsvC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1UtSXO7JYQx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}