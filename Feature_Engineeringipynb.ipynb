{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.What is a parameter?\n",
        "- A parameter is a variable that you define in the function or method definition to accept values (called arguments) when the function is called.\n",
        "\n",
        "#2. What is correlation?What does negative correlation mean?\n",
        "- Correlation is a statistical measure that shows the strength and direction of a relationship between two variables.\n",
        "\n",
        "- A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "\n",
        "#3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "- Machine Learning is a branch of Artificial Intelligence (AI) that enables computers to learn from data and improve their performance on tasks without being explicitly programmed.\n",
        "\n",
        " Main Components in Machine Learning:\n",
        "Here are the key components of any machine learning system:\n",
        "\n",
        "1. Data:The foundation of machine learning.Includes input features and (often) output labels.Types: Structured (tables), unstructured (images, text), etc.\n",
        "\n",
        "2. Model:A mathematical representation that maps input data to output.\n",
        "\n",
        "3. Algorithm:The procedure or method used to train the model.Determines how the model learns patterns from data.\n",
        "Examples: Gradient Descent, k-NN, SVM training, etc.\n",
        "\n",
        "4. Training\n",
        "The process where the model learns from the training data.The model adjusts its internal parameters to reduce errors.\n",
        "\n",
        "5. Testing / Evaluation\n",
        "After training, the model is tested on new data to evaluate its performance.\n",
        "Uses metrics like accuracy, precision, recall, F1-score, or mean squared error.\n",
        "\n",
        "6. Features\n",
        "The individual measurable properties or characteristics of the data.Good feature selection/extraction is crucial for accuracy.\n",
        "\n",
        "7. Labels (for supervised learning)\n",
        "The correct answers the model tries to predict during training.\n",
        "\n",
        "8. Loss Function / Cost Function\n",
        "A measure of how well the model’s predictions match the actual values.The goal of training is to minimize this loss.\n",
        "\n",
        "9. Optimization\n",
        "The method to adjust the model’s parameters to minimize the loss.Common methods: Gradient Descent, Adam, etc\n",
        "\n",
        "#4.How does loss value help in determining whether the model is good or not?\n",
        "- The loss value (also called cost) is a numerical measure of how wrong the model's predictions are compared to the actual values. It tells us how well or poorly the model is performing during training or testing.\n",
        "\n",
        "- A low loss value means the model’s predictions are close to the actual values → Good model.\n",
        "\n",
        "- A high loss value means the predictions are far from the actual values → Poor model.\n",
        "\n",
        "#5. What are continuous and categorical variables?\n",
        "- Continuous variables are numerical values that can take any value within a range, such as height, temperature, or income — they are measurable and infinite in precision. In contrast, categorical variables represent distinct groups or categories, like gender, color, or type of vehicle — they are not numerical in nature. Continuous variables are used in regression tasks, while categorical variables are often used in classification tasks. Understanding the type of variable helps in choosing the right analysis or machine learning model.\n",
        "\n",
        "#6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "- Most machine learning algorithms require numerical input, so categorical variables (like \"Male\"/\"Female\" or \"Red\"/\"Blue\") must be converted to numbers before training the model. This process is called encoding.\n",
        "\n",
        "#Common Techniques to Handle Categorical Variables:\n",
        "1. Label Encoding:\n",
        "Converts each category to a unique integer.\n",
        "Example: {\"Red\": 0, \"Green\": 1, \"Blue\": 2}\n",
        " Risk: Implies an order, which may not exist.\n",
        "Useful for: Tree-based models (e.g., Decision Trees, Random Forests)\n",
        "\n",
        "2. One-Hot Encoding:\n",
        "Creates separate binary columns for each category.\n",
        "Example: \"Red\" becomes [1, 0, 0], \"Green\" → [0, 1, 0]\n",
        "Prevents false order; preferred for most ML models.\n",
        "Can increase dimensionality if many categories.\n",
        "\n",
        "3. Ordinal Encoding:\n",
        "Like label encoding but used when order matters (e.g., {\"Low\": 0, \"Medium\": 1, \"High\": 2}).\n",
        "Good for: Ordered categories with meaningful rankings.\n",
        "\n",
        "4. Target Encoding (Mean Encoding)\n",
        "Replaces each category with the mean of the target variable for that category.\n",
        "Risk of overfitting, so usually combined with cross-validation.\n",
        "\n",
        "#7.What do you mean by training and testing a dataset??\n",
        "- In machine learning, a dataset is typically split into training and testing sets. The training set is used to teach the model by helping it learn patterns from the data. The testing set is used to evaluate how well the model performs on unseen data. This ensures the model can generalize and not just memorize the training data.\n",
        "\n",
        "#8.What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in Scikit-learn (a popular Python machine learning library) that provides tools to prepare and transform data before feeding it into a model. It includes functions for scaling, normalizing, encoding categorical variables, handling missing values, and more.\n",
        "\n",
        "#9.What is a Test set?\n",
        "- A test set is a portion of the dataset reserved to evaluate a machine learning model after it has been trained. It contains data the model has never seen before, allowing us to assess how well the model can generalize to new, unseen examples. By testing the model on this separate data, we get an unbiased estimate of its real-world performance and can detect issues like overfitting. The test set is crucial for validating that the model will work effectively beyond just the training data.\n",
        "\n",
        "#10. How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
        "- You can use train_test_split from sklearn.model_selection to easily split your dataset into training and testing sets.\n",
        "\n",
        "- from sklearn.model_selection import train_test_split\n",
        "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#How to Approach a Machine Learning Problem?\n",
        "\n",
        "1. Understand the Problem\n",
        "Define the goal, type of problem (classification, regression), and success metrics.\n",
        "\n",
        "2. Collect and Explore Data\n",
        "Gather data and perform exploratory data analysis (EDA) to understand patterns, missing values, and data quality.\n",
        "\n",
        "3. Preprocess and Clean Data\n",
        "Handle missing values, encode categorical variables, scale/normalize features.\n",
        "\n",
        "4. Split the Data\n",
        "Divide into training and testing (and sometimes validation) sets.\n",
        "\n",
        "5. Select and Train Model\n",
        "Choose an appropriate algorithm, train it on the training data.\n",
        "\n",
        "6. Evaluate Model\n",
        "Test the model on the test set using relevant metrics (accuracy, RMSE, etc.).\n",
        "\n",
        "7. Tune Hyperparameters\n",
        "Optimize model parameters for better performance (using Grid Search, Random Search).\n",
        "\n",
        "8. Deploy and Monitor\n",
        "Put the model into production and monitor its performance over time.\n",
        "\n",
        "#11. Why do we have to perform EDA before fitting a model to the data?\n",
        "- We perform Exploratory Data Analysis (EDA) before fitting a model because it helps us understand the data’s structure, patterns, and quality. EDA reveals important insights like missing values, outliers, feature distributions, and relationships between variables, which guide how to clean and preprocess the data. Without EDA, we might feed the model poor-quality or misleading data, leading to inaccurate or biased predictions. Essentially, EDA ensures we make informed decisions to build a more effective and reliable machine learning model\n",
        "\n",
        "#12.What is correlation?\n",
        "- Correlation is a statistical measure that shows the strength and direction of a relationship between two variables. It indicates how one variable changes when the other variable changes. The correlation coefficient ranges from -1 to +1\n",
        "\n",
        "#13. What does negative correlation mean?\n",
        "- Negative correlation means that two variables move in opposite directions—when one variable increases, the other decreases. The correlation coefficient for a negative correlation is between -1 and 0, where -1 indicates a perfect negative relationship. For example, if the amount of exercise goes up and body weight goes down, those two variables have a negative correlation. It shows an inverse relationship between the variables.\n",
        "\n",
        "#14. How can you find correlation between variables in Python?\n",
        "- Use libraries like pandas or NumPy.\n",
        "\n",
        "- Calculate the Pearson correlation coefficient, which measures linear correlation between two variables.\n",
        "\n",
        "- In pandas, use the .corr() method on a DataFrame to get correlations between all columns or between two specific columns.\n",
        "\n",
        "- In NumPy, use np.corrcoef() to compute the correlation coefficient matrix for arrays.\n",
        "\n",
        "- The output is a value between -1 and +1 indicating the strength and direction of the relationship\n",
        "\n",
        "#15.What is causation? Explain difference between correlation and causation with an example.\n",
        "- Causation means that one event or variable directly causes a change in another. In other words, a change in variable A produces a change in variable B.\n",
        "\n",
        "- Correlation means that two variables have a relationship and tend to move together, but one does not necessarily cause the other. It simply shows an association or pattern between variables. For example, ice cream sales and drowning incidents both increase during summer, so they are correlated because of the season, but ice cream sales do not cause drowning.\n",
        "\n",
        "- Causation means that one variable directly causes a change in another. It implies a cause-and-effect relationship. For instance, smoking causes lung cancer, where smoking is the actual reason for the increased risk of disease. Unlike correlation, causation proves that one event is the direct result of another\n",
        "\n",
        "#16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "- An optimizer is an algorithm used in machine learning and deep learning to adjust the model’s parameters (like weights) during training to minimize the loss function. The goal of the optimizer is to find the best values for these parameters so the model can make accurate predictions by reducing errors.\n",
        "\n",
        "#Different Types of Optimizers:\n",
        "\n",
        "- Gradient Descent (GD):\n",
        "The simplest optimizer that updates model parameters by moving them in the direction of the negative gradient of the loss function. It uses all training data at once for each update.\n",
        "Example: For linear regression, gradient descent updates weights step-by-step to reduce prediction error.\n",
        "\n",
        "- Stochastic Gradient Descent (SGD):\n",
        "Instead of using the entire dataset, SGD updates parameters using one training example at a time. This makes updates faster and adds noise that can help escape local minima.\n",
        "Example: When training a neural network on a large dataset, SGD updates weights after each example rather than waiting for the full batch.\n",
        "\n",
        "- Mini-batch Gradient Descent:\n",
        "A middle ground between GD and SGD, it updates parameters using small batches of data. This balances speed and stability.\n",
        "Example: Using batches of 32 samples to update weights during deep learning model training.\n",
        "\n",
        "- Adam (Adaptive Moment Estimation):\n",
        "A popular optimizer combining ideas from momentum and RMSProp, it adapts learning rates for each parameter individually based on first and second moments of gradients. This often leads to faster convergence.\n",
        "Example: Used widely in training deep learning models like CNNs and transformers.\n",
        "\n",
        "- RMSProp:\n",
        "This optimizer adapts the learning rate for each parameter based on a moving average of recent gradients’ magnitudes, helping with training on noisy or sparse data.\n",
        "Example: Effective in recurrent neural networks (RNNs) for sequence data.\n",
        "\n",
        "\n",
        "#17.What is sklearn.linear_model?\n",
        "- sklearn.linear_model is a module in the Scikit-learn library that contains classes and functions to implement linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
        "\n",
        "#18.What does model.fit() do? What arguments must be given?\n",
        "- model.fit() is a method used in machine learning libraries like Scikit-learn to train or fit a model on your data. When you call fit(), the model learns the relationship between the input features and the target variable by adjusting its internal parameters.\n",
        "\n",
        "#19.What does model.predict() do? What arguments must be given?\n",
        "- model.predict() is a method used to make predictions with a trained machine learning model. After you’ve trained your model using fit(), you use predict() to estimate the output (target variable) for new or unseen input data.\n",
        "\n",
        "#20.What are continuous and categorical variables?\n",
        "- Continuous variables are numeric variables that can take any value within a range. They represent measurements like height, weight, temperature, or time, where values can be decimals and are infinite within a range.\n",
        "\n",
        "- Categorical variables represent distinct groups or categories and take on a limited, fixed number of possible values. Examples include gender (male/female), colors (red, blue, green), or types of animals. They are often non-numeric or encoded into numbers for modeling.\n",
        "\n",
        "#21.What is feature scaling? How does it help in Machine Learning?\n",
        "- Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in your data so they have a similar scale. This typically involves transforming features to a specific range (like 0 to 1) or adjusting them to have a mean of zero and standard deviation of one.\n",
        "\n",
        "1. Improves model performance: Many algorithms (like gradient descent, SVM, KNN) perform better and converge faster when features are on a similar scale.\n",
        "\n",
        "2. Prevents dominance: Prevents features with larger numeric ranges from dominating those with smaller ranges.\n",
        "\n",
        "3. Helps distance-based models: Algorithms relying on distance (e.g., KNN, clustering) work correctly when features are scaled.\n",
        "\n",
        "4. Speeds up training: Leads to more stable and efficient training by reducing numerical instabilities.\n",
        "\n",
        "#22.How do we perform scaling in Python?\n",
        "-  1.Standardization (Z-score Scaling):\n",
        "Scales features to have a mean of 0 and standard deviation of 1.\n",
        "\n",
        "1. from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "2. scaler = StandardScaler()\n",
        "\n",
        "3. X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "- 2.Min-Max Scaling (Normalization):\n",
        "Scale features to a fixed range, usually 0 to 1.\n",
        "1. from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "2. scaler = MinMaxScaler()\n",
        "3. X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#23.What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in the Scikit-learn library that provides various tools and functions to preprocess and transform data before applying machine learning models. It includes methods for scaling, normalizing, encoding categorical variables, imputing missing values, and more.\n",
        "\n",
        "#24. How do we split data for model fitting (training and testing) in Python?\n",
        "- In Python, we split data into training and testing sets using the train_test_split function from Scikit-learn’s model_selection module. This helps evaluate the model’s performance on unseen data.\n",
        "\n",
        "1. Import the function:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "2. Use it to split your features (X) and target (y) into training and testing sets:\n",
        "\n",
        "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#25.Explain data encoding?\n",
        "- Data encoding is the process of converting categorical (non-numeric) data into a numerical format so that machine learning models can understand and process it. Since most algorithms require numerical input, encoding transforms categories like “red,” “blue,” or “green” into numbers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zwZHVWyPbGr3"
      }
    }
  ]
}